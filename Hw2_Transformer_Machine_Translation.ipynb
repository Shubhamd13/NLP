{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhamd13/NLP/blob/main/Hw2_Transformer_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# put your student name here, you will need to train the model that prints out your name in each loss\n",
        "# without the name, you will not be able to get points in train part\n",
        "STUDENT_NAME = \"Shubham Derhgawen\""
      ],
      "metadata": {
        "id": "jbYfCX1-csCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary\n",
        "Credit: https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "\n",
        "Download required packages, Imports libraries, sets random seeds."
      ],
      "metadata": {
        "id": "nwdhro5z9J_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torchaudio torchvision\n",
        "!pip install --force-reinstall torchtext==0.16.2\n",
        "!pip install portalocker>=2.0.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install numpy==1.26\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c2DRfGV-nyG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import spacy\n",
        "import warnings\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Set to False to skip notebook execution (e.g. for debugging)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ],
      "metadata": {
        "id": "19_RLM_39Vk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Model Architecture"
      ],
      "metadata": {
        "id": "GGPNygrg28AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Embedding (5pt)\n",
        "\n",
        "Defines the word-embedding layer that maps token indices to dense vectors.\n",
        "\n",
        "The shape of the Embedding should be: **[Vocab, Embedding_size]**"
      ],
      "metadata": {
        "id": "av_WHKp6CayD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        ######### add your code here\n",
        "        self.emb =\n",
        "        #########\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "3Ikf1cvYCbVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Position Encoding (15pt)\n",
        "\n",
        "Implements sinusoidal positional encodings so the model can attend to token order without recurrence.\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
        "$$\n",
        "\n",
        "1. `pos` is the position `i` is the dimension.\n",
        "2. Each dimension of the position encoding corresponds to a sinusoid.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LCqKUlcZCfOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        ######### add your code here\n",
        "        div_term =\n",
        "        pe[:, 0::2] =\n",
        "        pe[:, 1::2] =\n",
        "        #########\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "EuTdaT08CfUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 LayerNorm (10pt)\n",
        "Provides a custom Layer Normalization module to stabilise training by normalising hidden states across features.\n",
        "\n",
        "We will follow the basic setting in [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "\n",
        "$$\n",
        "y = \\frac{x - E[x]}{Var[x] + \\epsilon}*\\gamma + \\beta\n",
        "$$\n",
        "\n",
        "where:\n",
        "1. `ϵ`: a value added to the denominator for numerical stability\n",
        "2. `γ` & `β`: weight (initialized to 1) and bias (initalized to 0)."
      ],
      "metadata": {
        "id": "lfornq-8_jrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        ######### add your code here\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 =\n",
        "        #########\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        ######### add your code here\n",
        "        result =\n",
        "        #########\n",
        "        return result"
      ],
      "metadata": {
        "id": "Pukl4c0j_njP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 SubLayer Connection (10pt)\n",
        "\n",
        "Wraps a residual connection around a sub-layer (e.g., attention or FFN). Right here, we apply the pre-norm setup.\n",
        "\n",
        "The output of each sub-layer is:\n",
        "$$\n",
        "x + Sublayer(LayerNorm(x))\n",
        "$$\n",
        "where:\n",
        "1. `Sublayer(x)` is the function implemented by the sub-layer itself.(Attn/MLP)"
      ],
      "metadata": {
        "id": "pUoz2hQ6AG9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        ######### add your code here\n",
        "        return\n",
        "        #########"
      ],
      "metadata": {
        "id": "9jg9mHK9AHVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Multi-Head Attention (20pt)\n",
        "\n",
        "Implements scaled dot-product self-attention, splits it into h parallel “heads,” and concatenates the results back together.\n",
        "\n",
        "For each head:\n",
        "$$\n",
        "Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "Multi-Head Attention:\n",
        "$$\n",
        "MultiHead(Q, K, V) = Concat(head_1,\\dots, head_h)W^O \\\\\n",
        "where\\quad head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "LtUUG8X9A3xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    ######### add your code here\n",
        "    scores =\n",
        "    #########\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    ######### add your code here\n",
        "    p_attn =\n",
        "    #########\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "8sVsO2_xAtLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "        self.linears = nn.Linear(d_model, d_model)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        ######### add your code here\n",
        "        query = self.Wq(query).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key =\n",
        "        value =\n",
        "        #########\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(\n",
        "            query, key, value, mask=mask, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        ######### add your code here\n",
        "        x =\n",
        "        #########\n",
        "\n",
        "        del query\n",
        "        del key\n",
        "        del value\n",
        "        return self.linears(x)"
      ],
      "metadata": {
        "id": "KKXVrr7EAv3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 FeedFordward Network (FFN) (5pt)\n",
        "Creates the two-layer position-wise feed-forward network applied after each attention block."
      ],
      "metadata": {
        "id": "yL0oZM8eA-LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        ######### add your code here\n",
        "        self.w_2 =\n",
        "        #########\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))"
      ],
      "metadata": {
        "id": "_Z_Njzq4A-eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Encoder"
      ],
      "metadata": {
        "id": "QOWx1p27-wrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Encoder Layer\n",
        "\n",
        "Defines a single Encoder block consisting of a Multi-Head Self-Attention layer and an FFN, each wrapped in Add & Norm."
      ],
      "metadata": {
        "id": "SVODKZfa_7rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "wJWBzFyt-64U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Encoder Stacks (10pt)\n",
        "Stacks N Encoder blocks to form the full Encoder."
      ],
      "metadata": {
        "id": "eLfbujxM_411"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        ######### add your code here\n",
        "\n",
        "        #########\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "Zp6UtcVC-0El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Decoder"
      ],
      "metadata": {
        "id": "A-yz1UvNAaD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Decoder Layer (5pt)\n",
        "\n",
        "Details a single Decoder block containing masked self-attention, cross-attention, and an FFN, each with residual Add & Norm."
      ],
      "metadata": {
        "id": "KxTvioDZAibw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        ######### add your code here\n",
        "\n",
        "        #########\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "EgWip7SNAkqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Decoder Stacks (10pt)\n",
        "Stacks N Decoder blocks to build the complete Decoder."
      ],
      "metadata": {
        "id": "naHuN-v8Ae8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        ######### add your code here\n",
        "\n",
        "        #########\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "abuJsIP5AZbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Combines the Encoder and Decoder into a full Transformer model and adds a linear projection to produce logits over the target vocabulary."
      ],
      "metadata": {
        "id": "gQmMX48iCn0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many\n",
        "    other models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "CAlGGs3DCvyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(\n",
        "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
        "):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab),\n",
        "    )\n",
        "\n",
        "    # This was important from their code.\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "gn3geYnbCpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train (10pt)\n",
        "\n",
        "Prepares a small EN→DE dataset, builds dataloaders, defines the loss, optimizer, and training loop, then runs a quick demonstration training epoch followed by an inference example."
      ],
      "metadata": {
        "id": "qFaaJLjmErAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import spacy\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def subsequent_mask(size, device=None):\n",
        "    # Mask has 1s in the *allowed* (≤ i) positions, 0 elsewhere\n",
        "    attn_shape = (1, size, size)\n",
        "    mask = torch.triu(torch.ones(attn_shape, dtype=torch.bool, device=device), diagonal=1)\n",
        "    return ~mask      # invert so future positions are 0\n",
        "\n",
        "# Load tokenizers (ensure you've downloaded the models:\n",
        "# python -m spacy download en_core_web_sm && python -m spacy download de_core_news_sm)\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Prepare dataset (only 500 samples for a quick test)\n",
        "raw_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
        "raw_data = list(raw_iter)[:500]\n",
        "test_iter = Multi30k(split='valid', language_pair=('en', 'de'))\n",
        "test_data = list(test_iter)[:10]\n",
        "\n",
        "# Build vocabularies with special tokens\n",
        "SRC_SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "TGT_SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_src = build_vocab_from_iterator((tokenize_en(pair[0]) for pair in raw_data), specials=SRC_SPECIALS)\n",
        "vocab_tgt = build_vocab_from_iterator((tokenize_de(pair[1]) for pair in raw_data), specials=TGT_SPECIALS)\n",
        "\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])\n",
        "\n",
        "SRC_PAD_IDX = vocab_src['<pad>']\n",
        "TGT_PAD_IDX = vocab_tgt['<pad>']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in batch:\n",
        "        src_tokens = [vocab_src['<bos>']] + [vocab_src[token] for token in tokenize_en(src)] + [vocab_src['<eos>']]\n",
        "        tgt_tokens = [vocab_tgt['<bos>']] + [vocab_tgt[token] for token in tokenize_de(tgt)] + [vocab_tgt['<eos>']]\n",
        "        src_batch.append(torch.tensor(src_tokens, dtype=torch.long))\n",
        "        tgt_batch.append(torch.tensor(tgt_tokens, dtype=torch.long))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=SRC_PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=TGT_PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_loader = DataLoader(raw_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = make_model(len(vocab_src), len(vocab_tgt), N=2, d_model=64, d_ff=128, h=4, dropout=0.1).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.NLLLoss(ignore_index=TGT_PAD_IDX)\n",
        "\n",
        "\n",
        "model.train()\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_loss = 0\n",
        "    for i, (src, tgt) in enumerate(train_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        src_mask = (src != SRC_PAD_IDX).unsqueeze(-2)\n",
        "        tgt_pad_mask = (tgt_input != TGT_PAD_IDX).unsqueeze(-2)\n",
        "        tgt_mask     = tgt_pad_mask & subsequent_mask(tgt_input.size(1), device=tgt_input.device)\n",
        "        ######### add your code here\n",
        "        out =\n",
        "        #########\n",
        "        logits = model.generator(out)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"{STUDENT_NAME} Epoch {epoch} | Loss: {total_loss/(i+1):.4f} | Time: {time.time()-start_time:.2f}s\")"
      ],
      "metadata": {
        "id": "_SU86-VkEuOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "2IYPYTNoa0KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding for inference\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1, dtype=torch.long).fill_(start_symbol).to(device)\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = subsequent_mask(ys.size(1)).to(device)\n",
        "        out = model.decode(memory, src_mask, ys, tgt_mask)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        next_word = torch.argmax(prob, dim=1).item()\n",
        "        ys = torch.cat([ys, torch.tensor([[next_word]], dtype=torch.long).to(device)], dim=1)\n",
        "        if next_word == vocab_tgt['<eos>']:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "model.eval()\n",
        "for index, example in enumerate(test_data[:10]):\n",
        "    src = torch.tensor(\n",
        "        [vocab_src['<bos>']] + [vocab_src[t] for t in tokenize_en(example[0])] + [vocab_src['<eos>']],\n",
        "        dtype=torch.long\n",
        "    ).unsqueeze(0).to(device)\n",
        "    src_mask = (src != SRC_PAD_IDX).unsqueeze(-2)\n",
        "    translation = greedy_decode(model, src, src_mask, max_len=50, start_symbol=vocab_tgt['<bos>'])\n",
        "    tokens = [vocab_tgt.get_itos()[idx] for idx in translation.squeeze().tolist()]\n",
        "\n",
        "    print(\"Source:\", example[0])\n",
        "    print(\"Reference:\", example[1])\n",
        "    print(\"Predicted:\", \" \".join(tokens))\n",
        "    print('*******'*50)"
      ],
      "metadata": {
        "id": "4eXzi_Kaa0Uf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N6qtm5U7XhPf",
        "OmcAQ6FtXvTU",
        "B_ugsTb1X59q",
        "l4TdjaIfZGJo",
        "aBUi-JUwgT_i",
        "ggOBABeGgcfH",
        "8Q6zC2pambpf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhamd13/NLP/blob/main/4_1_Pre_Training_Student_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 01: Masked Language Modeling"
      ],
      "metadata": {
        "id": "TeObwDKxSVax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we have the following two sentences:\n",
        "\n",
        "$\\text{Target Text: \"I am a student of UC Riverside\"}$\n",
        "\n",
        "$\\text{Input Text: \"I am a <MASK> of UC Riverside\"}$\n",
        "\n",
        "Our goal is to train an encoder using masked language modeling objective. During training, we will feed the input text (with a masked word) into the encoder and see what it predicts at the masked position. Then, we will compare the prediction to the correct word (\"student\") to calculate the training loss.\n",
        "\n",
        "To keep things simple, we will use a basic encoder architecture that follows this flow:\n",
        "\n",
        "$\n",
        "\\text{Input Embedding} \\rightarrow \\text{Self-Attention} \\rightarrow \\text{Linear Projection to Vocabulary} \\rightarrow \\text{Loss Calculation}\n",
        "$\n",
        "\n",
        "We are skipping some of the usual components like MLP layers, residual connections, and layer normalization for simplicity.\n"
      ],
      "metadata": {
        "id": "Zu_VjNTUUEzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Vocabulary, Input and Target token ids (Do not change)"
      ],
      "metadata": {
        "id": "N6qtm5U7XhPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "\n",
        "words = [\"staff\", \"I\", \"student\", \"<EOS>\", \"UC\", \"am\", \"a\", \"<MASK>\", \"Riverside\", \"of\"]\n",
        "vocab = {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "target_tokens = [\"I\", \"am\", \"a\", \"student\", \"of\", \"UC\", \"Riverside\"]\n",
        "input_tokens =  [\"I\", \"am\", \"a\", \"<MASK>\", \"of\", \"UC\", \"Riverside\"]\n",
        "\n",
        "target_ids = [vocab[t] for t in target_tokens] # get target token ids from vocabulary\n",
        "input_ids = [vocab[t] for t in input_tokens]   # get input token ids from vocabulary\n",
        "\n",
        "print(\"Target Ids: \", target_ids)\n",
        "print(\"Input Ids:  \", input_ids)\n",
        "\n",
        "# Find the position of <MASK> token\n",
        "mask_position = input_tokens.index(\"<MASK>\")\n",
        "\n",
        "# We only need to calculate loss for the target tokens at masked position, we do not want to calculate loss for other tokens and so we set -100\n",
        "target_ids = [-100 if i != mask_position else tid for i, tid in enumerate(target_ids)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNa490MDsZTA",
        "outputId": "3bc6dc12-5fc8-42b2-e5b0-4ff3e935e2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Ids:  [1, 5, 6, 2, 9, 4, 8]\n",
            "Input Ids:   [1, 5, 6, 7, 9, 4, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Input Embedding Matrix (Do not change)"
      ],
      "metadata": {
        "id": "OmcAQ6FtXvTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.array([\n",
        "    [0.1, 0.2, 0.3],     # I\n",
        "    [0.4, 0.3, 0.1],     # am\n",
        "    [1, 1, 1.],          # a\n",
        "    [0., 0., 0.],        # <MASK>\n",
        "    [0.1, 0.1, 1.0],     # of\n",
        "    [0., 0., 0.],        # UC\n",
        "    [0.5, 0.5, 0.5],     # Riverside\n",
        "])\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0lXK2u8voO9",
        "outputId": "55e23155-03ac-48b6-c416-e0bba9654fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Self Attention Calculation (Do not change)"
      ],
      "metadata": {
        "id": "B_ugsTb1X59q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "Fo simplicity, we assume -\n",
        "\n",
        "$Q = Embed_{matrix} * 1.25$\n",
        "\n",
        "$K = Embed_{matrix} * 1.88$\n",
        "\n",
        "$V = Embed_{matrix} * 1.03$"
      ],
      "metadata": {
        "id": "8lhFdyeyYI5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Q, K, V\n",
        "Q = embedding_matrix * 1.25\n",
        "K = embedding_matrix * 1.88\n",
        "V = embedding_matrix * 1.03\n",
        "\n",
        "# Calculate QK^T\n",
        "attention_scores = Q @ K.T  # shape: (7, 7)\n",
        "\n",
        "# Perform scaling: find the dimension of keys in the Key matrix and divide scores by the square root of the key dimension.\n",
        "dk = K.shape[1]\n",
        "scaled_attention_scores = attention_scores / np.sqrt(dk)\n",
        "\n",
        "# Compute Softmax. Softmax converts raw attention scores into probabilities.\n",
        "attention_weights = np.exp(scaled_attention_scores)\n",
        "attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n",
        "\n",
        "# Multiply attention weights with value matrix\n",
        "self_attention_output = attention_weights @ V\n",
        "self_attention_output"
      ],
      "metadata": {
        "id": "iQTZhDx-vy1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Linear Projection to Vocabulary (Do not change)"
      ],
      "metadata": {
        "id": "l4TdjaIfZGJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, here we assume that the encoder output is the self attention output.\n",
        "\n",
        "Assume we have a weight matrix for the linear layer: $ùëä_{out} ‚àà ùëÖ^{3√ó10}$ ‚Üí here, embedding size = 3 and vocab size = 10\n",
        "\n",
        "$logits = \\text{self attention}_{output} \\; (dot \\; product) \\; W_{out} $\n"
      ],
      "metadata": {
        "id": "9ERuzMFzZh1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we use the encoder output to get logits for each word in the vocabulary\n",
        "# We do this by using a linear layer.\n",
        "# For each token, output will be logits over all the words in the vocabulary, since we have 10 words in the vocabulary, output shape will be the vocabulary size for each token\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "A = embedding_matrix.shape[1]\n",
        "B = len(vocab)\n",
        "\n",
        "V_W = np.random.randn(A, B) # weight matrix for the linear layer\n",
        "logits = np.dot(self_attention_output, V_W)\n",
        "\n",
        "print(\"Logits:\\n\", logits)"
      ],
      "metadata": {
        "id": "G1qEFNTNw2Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Calculate Cross Entropy Loss"
      ],
      "metadata": {
        "id": "J_Ljqn-DbABQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for binary cross-entropy loss is:\n",
        "\n",
        "$\\mathcal{L} = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]$\n",
        "\n",
        "Here,\n",
        "\n",
        "$y$ = true label (either 0 or 1)\n",
        "\n",
        "$\\hat{y}$ =  model‚Äôs predicted probability\n",
        "\n",
        "In our example, since \"student\" is the target word, we treat it like a positive class ‚Äî so we set $y=1$.\n",
        "\n",
        "So the loss becomes -\n",
        "\n",
        "$\\mathcal{L} = - \\log(\\hat{y})$"
      ],
      "metadata": {
        "id": "Poz_LwjxbDwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits, target_ids, ignore_index=-100):\n",
        "    logits = np.array(logits)          # shape: [seq_len, vocab_size]\n",
        "    target_ids = np.array(target_ids)  # shape: [seq_len]\n",
        "\n",
        "    # Step 1: Apply softmax to get probabilities\n",
        "    exp_logits = np.exp(logits)\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "    # Step 2: Select probabilities of the target tokens (only where target != ignore_index)\n",
        "    loss_values = []\n",
        "    for i, target in enumerate(target_ids):\n",
        "        if target == ignore_index:\n",
        "            continue\n",
        "\n",
        "        temp_prob = probs[i]\n",
        "\n",
        "        ###<-- Write code here (Q4)\n",
        "        temp_idx =                ## find the maximum probability index\n",
        "        ###\n",
        "\n",
        "\n",
        "        print(\"Target Word: \", words[target])\n",
        "        print(\"Predicted Word: \", words[temp_idx])\n",
        "\n",
        "        # Step 3: Calculate Loss\n",
        "        p = temp_prob[target] # probability of the target token id\n",
        "\n",
        "\n",
        "        ###<-- Write code here (Q5)\n",
        "        loss =\n",
        "        ###\n",
        "\n",
        "\n",
        "        loss_values.append(loss)\n",
        "\n",
        "    return sum(loss_values)\n",
        "\n",
        "loss = cross_entropy_loss(logits, target_ids, ignore_index=-100)\n",
        "print(\"Masked LM Loss:\", loss)"
      ],
      "metadata": {
        "id": "1ZqhwzAuBX8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 02: Next Word Prediction"
      ],
      "metadata": {
        "id": "rtNjb50urrA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume that we have the following token sequences:\n",
        "\n",
        "$\\text{Input¬†Tokens:  [\"I\",¬†\"am\",¬†\"a\",¬†\"student\",¬†\"of\",¬†\"UC\",¬†\"Riverside\"]}$\n",
        "\n",
        "$\\text{Target¬†Tokens:¬†[\"am\",¬†\"a\",¬†\"student\",¬†\"of\",¬†\"UC\",¬†\"Riverside\",¬†\"<EOS>\"]}$\n",
        "\n",
        "Our goal is to train a decoder using the next word prediction (also called causal language modeling) objective. During training, we feed the input tokens into the decoder one by one, and at each step, the model tries to predict the next token in the sequence.\n",
        "\n",
        "For example:\n",
        "\n",
        "Given input $\\text{\"I\"}$, the model should predict $\\text{\"am\"}$.\n",
        "\n",
        "Given $\\text{\"I am\"}$, it should predict $\\text{\"a\"}$ and so on.\n",
        "\n",
        "The final target is the special token $\\text{<EOS>}$ that marks the end of the sentence.\n",
        "\n",
        "To keep things simple, we will use a basic decoder architecture that follows this flow:\n",
        "\n",
        "$\\text{Input¬†Embedding ‚Üí Masked¬†Self-Attention ‚Üí Linear¬†Projection¬†to¬†Vocabulary ‚Üí Loss¬†Calculation}$\n",
        "\n",
        "\n",
        "We apply a causal mask (lower triangular) in the self-attention layer to make sure the model only attends to previous tokens and not future ones. For simplicity, we are skipping components like MLP layers, residual connections, and layer normalization steps."
      ],
      "metadata": {
        "id": "90Jf9yhTdpHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Vocabulary, Input and Target Token IDs (Do not change)"
      ],
      "metadata": {
        "id": "aBUi-JUwgT_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "\n",
        "# Vocabulary construction\n",
        "words = [\"staff\", \"I\", \"student\", \"<EOS>\", \"UC\", \"am\", \"a\", \"<MASK>\", \"Riverside\", \"of\"]\n",
        "vocab = {word: idx for idx, word in enumerate(words)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "input_tokens =  [\"I\", \"am\", \"a\", \"student\", \"of\", \"UC\", \"Riverside\"]\n",
        "target_tokens = [\"am\", \"a\", \"student\", \"of\", \"UC\", \"Riverside\", \"<EOS>\"]\n",
        "\n",
        "input_ids = [vocab[t] for t in input_tokens]\n",
        "target_ids = [vocab[t] for t in target_tokens]\n",
        "\n",
        "print(\"Input Ids:  \", input_ids)\n",
        "print(\"Target Ids: \", target_ids)\n",
        "\n",
        "seq_len = len(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEli3ybGSB0g",
        "outputId": "8dc0dd77-1207-4340-eeed-50d3b42d72a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Ids:   [1, 5, 6, 2, 9, 4, 8]\n",
            "Target Ids:  [5, 6, 2, 9, 4, 8, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Input Embedding Matrix (Do not change)"
      ],
      "metadata": {
        "id": "ggOBABeGgcfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.array([\n",
        "    [1., 0., 0.],     # I\n",
        "    [0., 1., 0.],     # am\n",
        "    [0., 0., 1.],     # a\n",
        "    [1., 1., 0.],     # student\n",
        "    [1., 0., 1.],     # of\n",
        "    [0., 1., 1.],     # UC\n",
        "    [0.5, 0.5, 0.5],  # Riverside\n",
        "])\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTa9luWGSKAZ",
        "outputId": "d76f5306-31e2-4f70-e22d-c6a63bfbe691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Self Attention Calculation with Masking"
      ],
      "metadata": {
        "id": "lH2R8030gmqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Step 1. Query, Key, and Value Computation:}$\n",
        "\n",
        "$Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V$\n",
        "\n",
        "$\\textbf{Step 2. Attention Scores:}$\n",
        "\n",
        "$\\text{Attention Scores} = \\frac{Q K^T}{\\sqrt{d}}$\n",
        "\n",
        "$\\textbf{Step 3. Causal Masking:}$\n",
        "\n",
        "To enforce causal attention (only attending to past and current tokens), we apply a causal mask \\( M \\) to the attention scores matrix. The causal mask is defined as:\n",
        "\n",
        "$ M_{ij} =\n",
        "\\begin{cases}\n",
        "-\\infty & \\text{if } j > i \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "The attention scores are updated by adding the mask:\n",
        "\n",
        "$\\text{Masked Attention Scores} = \\text{Attention Scores} + M$\n",
        "\n",
        "$\\textbf{Step 4. Softmax:}$\n",
        "\n",
        "$\\text{Attention Weights} = \\text{Softmax}(\\text{Masked Attention Scores})$\n",
        "\n",
        "$\\textbf{Step 5. Weighted Sum of Values:}$\n",
        "\n",
        "$\\text{Attention Output} = \\text{Attention Weights} \\cdot V$\n",
        "\n",
        "$\\textbf{Final Formula:}$\n",
        "\n",
        "$\n",
        "\\text{Attention Output} = \\text{Softmax}\\left( \\frac{Q K^T}{\\sqrt{d}} + M \\right) \\cdot V$\n"
      ],
      "metadata": {
        "id": "6XITRj5-hA2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q, K, V\n",
        "Q = embedding_matrix * 1.25\n",
        "K = embedding_matrix * 1.88\n",
        "V = embedding_matrix * 1.03\n",
        "\n",
        "# Step 2: Calculate Attention Scores\n",
        "attn_scores = Q @ K.T\n",
        "dk = K.shape[1]\n",
        "scaled_attention_scores = attention_scores / np.sqrt(dk)\n",
        "print(\"Attention Scores Without Causal Masking:\\n\\n\", np.array(scaled_attention_scores))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Step 3a: Create Causal Mask only attend to past & current\n",
        "causal_mask = []\n",
        "for i in range(seq_len):\n",
        "    row = []\n",
        "    for j in range(seq_len):\n",
        "        if j > i:\n",
        "            row.append(float('-inf'))  # block future tokens\n",
        "        else:\n",
        "            ###<--- Write code here  (Q7: allow current & past tokens)\n",
        "            row.append(0.0)\n",
        "\n",
        "            ###\n",
        "\n",
        "    causal_mask.append(row)\n",
        "causal_mask = np.array(causal_mask)\n",
        "\n",
        "print(\"Attention Mask:\\n\\n\", np.array(causal_mask))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Step 3b: Apply Causal Mask to Attention Scores\n",
        "masked_attn_scores = scaled_attention_scores + causal_mask\n",
        "print(\"Attention Scores With Causal Masking:\\n\\n\", np.array(masked_attn_scores))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Step 4: Softmax\n",
        "def softmax(x):\n",
        "    exp_logits = np.exp(x)\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "    return probs\n",
        "\n",
        "attn_weights = softmax(masked_attn_scores)\n",
        "\n",
        "# Step 5: Weighted Sum of Values\n",
        "attention_output = attn_weights @ V\n",
        "print(\"Final Attention Output:\\n\\n\", np.array(attention_output))"
      ],
      "metadata": {
        "id": "1xg7mo61SIPc",
        "outputId": "0c8723ce-e3bb-4a5d-ea1a-ee0979b8d1d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores Without Causal Masking:\n",
            "\n",
            " [[0.19 0.18 0.81 0.   0.45 0.   0.41]\n",
            " [0.18 0.35 1.09 0.   0.23 0.   0.54]\n",
            " [0.81 1.09 4.07 0.   1.63 0.   2.04]\n",
            " [0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.45 0.23 1.63 0.   1.38 0.   0.81]\n",
            " [0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.41 0.54 2.04 0.   0.81 0.   1.02]]\n",
            "==================================================\n",
            "Attention Mask:\n",
            "\n",
            " [[  0. -inf -inf -inf -inf -inf -inf]\n",
            " [  0.   0. -inf -inf -inf -inf -inf]\n",
            " [  0.   0.   0. -inf -inf -inf -inf]\n",
            " [  0.   0.   0.   0. -inf -inf -inf]\n",
            " [  0.   0.   0.   0.   0. -inf -inf]\n",
            " [  0.   0.   0.   0.   0.   0. -inf]\n",
            " [  0.   0.   0.   0.   0.   0.   0.]]\n",
            "==================================================\n",
            "Attention Scores With Causal Masking:\n",
            "\n",
            " [[0.19 -inf -inf -inf -inf -inf -inf]\n",
            " [0.18 0.35 -inf -inf -inf -inf -inf]\n",
            " [0.81 1.09 4.07 -inf -inf -inf -inf]\n",
            " [0.   0.   0.   0.   -inf -inf -inf]\n",
            " [0.45 0.23 1.63 0.   1.38 -inf -inf]\n",
            " [0.   0.   0.   0.   0.   0.   -inf]\n",
            " [0.41 0.54 2.04 0.   0.81 0.   1.02]]\n",
            "==================================================\n",
            "Final Attention Output:\n",
            "\n",
            " [[1.03 0.   0.  ]\n",
            " [0.47 0.56 0.  ]\n",
            " [0.04 0.05 0.95]\n",
            " [0.52 0.52 0.26]\n",
            " [0.52 0.18 0.72]\n",
            " [0.52 0.52 0.52]\n",
            " [0.35 0.29 0.71]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Linear Projection to Vocabulary (Do not change)"
      ],
      "metadata": {
        "id": "8Q6zC2pambpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, here we assume that the decoder output is the self attention output.\n",
        "\n",
        "Assume we have a weight matrix for the linear layer: $ùëä_{out} ‚àà ùëÖ^{3√ó10}$ ‚Üí here, embedding size = 3 and vocab size = 10\n",
        "\n",
        "$logits = \\text{self attention}_{output} \\; (dot \\; product) \\; W_{out} $\n"
      ],
      "metadata": {
        "id": "-Q9ie36emcLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we use the decoder output to get logits for each word in the vocabulary\n",
        "# We do this by using a linear layer.\n",
        "# For each token, output will be logits over all the words in the vocabulary, since we have 10 words in the vocabulary, output shape will be the vocabulary size for each token\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "A = embedding_matrix.shape[1]\n",
        "B = len(vocab)\n",
        "\n",
        "V_W = np.random.randn(A, B) # weight matrix for the linear layer\n",
        "logits = np.dot(self_attention_output, V_W)\n",
        "\n",
        "print(\"Logits:\\n\", logits)"
      ],
      "metadata": {
        "id": "awRz5e6GSNeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Calculate Cross Entropy Loss"
      ],
      "metadata": {
        "id": "9Yxj7z-Jm-de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each token position, the loss is -\n",
        "\n",
        "$\\mathcal{L} = - \\log(\\hat{y})$"
      ],
      "metadata": {
        "id": "ZS7kL8ftm_YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(pred_logits, target_index):\n",
        "    exp_logits = np.exp(pred_logits)\n",
        "    probs = exp_logits / np.sum(exp_logits, keepdims=True)\n",
        "\n",
        "    ###<--- Write code here (Q9)\n",
        "    log_prob = -np.log(probs[target_index])                 ## -log(probability of target index)\n",
        "    ###\n",
        "\n",
        "    return log_prob\n",
        "\n",
        "losses = []\n",
        "for i in range(seq_len):\n",
        "    loss = cross_entropy(logits[i], target_ids[i])\n",
        "    print(f\"Token: '{words[input_ids[i]]}' ‚Üí Target: '{words[target_ids[i]]}', Loss: {round(loss, 4)}\")\n",
        "    losses.append(loss)\n",
        "\n",
        "total_loss = sum(losses) / len(losses)\n",
        "print(\"\\nTotal Loss:\", round(total_loss, 4))"
      ],
      "metadata": {
        "id": "S7Et082am-KR",
        "outputId": "0d8887ea-8dfe-4ff0-e9cf-f52e4319efc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: 'I' ‚Üí Target: 'am', Loss: 2.5084\n",
            "Token: 'am' ‚Üí Target: 'a', Loss: 2.6275\n",
            "Token: 'a' ‚Üí Target: 'student', Loss: 1.6803\n",
            "Token: 'student' ‚Üí Target: 'of', Loss: 2.6212\n",
            "Token: 'of' ‚Üí Target: 'UC', Loss: 3.5866\n",
            "Token: 'UC' ‚Üí Target: 'Riverside', Loss: 2.9099\n",
            "Token: 'Riverside' ‚Üí Target: '<EOS>', Loss: 3.5235\n",
            "\n",
            "Total Loss: 2.7796\n"
          ]
        }
      ]
    }
  ]
}
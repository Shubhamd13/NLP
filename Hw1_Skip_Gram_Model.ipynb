{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhamd13/NLP/blob/main/Hw1_Skip_Gram_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# HW1: Skip-Gram Model From Scratch"
      ],
      "metadata": {
        "id": "UPtKmzpAXQxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# put your student name here, you will need to train the model that prints out your name in each loss\n",
        "# without the name, you will not be able to get points in train part\n",
        "STUDENT_NAME = \"\""
      ],
      "metadata": {
        "id": "XeDg0qkY7u0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section imports necessary Python libraries such as NumPy and the random module. These libraries are essential for generating random training data and managing numerical computations."
      ],
      "metadata": {
        "id": "yRPgQWm-shsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "PYy9Ou1WXNsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Create Corpus\n",
        "\n",
        "In this section, you should first define a custom vocabulary composed of universities in Southern California (SoCal) and Northern California (NorCal). It generates a synthetic corpus by randomly pairing elements within each regional list to simulate word-context relationships."
      ],
      "metadata": {
        "id": "W-9_3Dk9V47g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SoCal = [\"UCR\", \"UCLA\", \"UCSD\", \"UCI\", \"UCSB\"]\n",
        "NorCal = [\"UCB\", \"UCSC\", \"UCD\", \"UCSF\", \"UCM\"]\n",
        "vocab = SoCal + NorCal\n",
        "\n",
        "corpus = []\n",
        "\n",
        "# Create Corpus\n",
        "for i in range(20):\n",
        "  random.shuffle(SoCal)\n",
        "  corpus.append((SoCal[0], SoCal[1]))\n",
        "\n",
        "for i in range(20):\n",
        "  random.shuffle(NorCal)\n",
        "  corpus.append((NorCal[0], NorCal[1]))\n",
        "\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 2\n",
        "learning_rate = 0.02\n",
        "num_neg_samples = 2\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "qkXyNoHOV6rF",
        "outputId": "eb876792-b263-4931-9ff2-7616f81002a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('UCSD', 'UCLA'), ('UCSD', 'UCR'), ('UCR', 'UCSD'), ('UCI', 'UCR'), ('UCR', 'UCSB'), ('UCSB', 'UCLA'), ('UCLA', 'UCSB'), ('UCI', 'UCLA'), ('UCI', 'UCSB'), ('UCLA', 'UCSB'), ('UCR', 'UCI'), ('UCI', 'UCLA'), ('UCI', 'UCR'), ('UCSB', 'UCSD'), ('UCI', 'UCSD'), ('UCSD', 'UCSB'), ('UCLA', 'UCSD'), ('UCSB', 'UCLA'), ('UCR', 'UCI'), ('UCLA', 'UCSD'), ('UCSF', 'UCD'), ('UCB', 'UCD'), ('UCM', 'UCSC'), ('UCM', 'UCD'), ('UCSF', 'UCB'), ('UCB', 'UCSC'), ('UCSC', 'UCD'), ('UCSC', 'UCB'), ('UCB', 'UCSF'), ('UCSF', 'UCD'), ('UCM', 'UCSC'), ('UCSC', 'UCD'), ('UCM', 'UCSC'), ('UCD', 'UCM'), ('UCM', 'UCB'), ('UCB', 'UCD'), ('UCM', 'UCB'), ('UCB', 'UCD'), ('UCB', 'UCSC'), ('UCSF', 'UCB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Define Visualization Function\n",
        "\n",
        "Implement plotting utilities for visualizing training loss, embedding space, and cosine similarity between word vectors. (No Need to Change.)\n"
      ],
      "metadata": {
        "id": "yeMj_MLPc98s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss over epochs\n",
        "def plot_loss_over_epochs(loss_values, epochs):\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  plt.plot(range(1, epochs + 1), loss_values, marker='o')\n",
        "  plt.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Average Loss')\n",
        "  plt.title('Loss over Epochs')\n",
        "  plt.show()\n",
        "\n",
        "# Plot Embedding\n",
        "def plot_embedding(matrix, vocab):\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  x = matrix[:, 0]\n",
        "  y = matrix[:, 1]\n",
        "  plt.scatter(x, y)\n",
        "  for i, word in enumerate(vocab):\n",
        "    plt.annotate(word, xy=(x[i], y[i]))\n",
        "\n",
        "  plt.title(\"Embedding Visualization\")\n",
        "  plt.show()\n",
        "\n",
        "# Plot Similary score\n",
        "def plot_cosine_similarity(matrix, vocab):\n",
        "\n",
        "  def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "  similarities = np.array([\n",
        "      cosine_similarity(matrix[0], emb) for emb in matrix\n",
        "  ])\n",
        "\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  plt.bar(range(len(similarities)), similarities)\n",
        "  plt.xticks(range(len(similarities)), vocab)\n",
        "  plt.ylim(-1.1, 1.1)\n",
        "  plt.ylabel(\"Cosine Similarity\")\n",
        "  plt.title(\"Cosine Similarity to UCR\")\n",
        "  plt.grid(axis='y')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "_uO3Q9uFc9E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Define trainable parameters (10pt)\n",
        "\n",
        "Use Numpy to initialize two trainable embedding matrices — one for context words and one for target words — with small random values.\n",
        "\n",
        "Here is the size for each Matrix:\n",
        "\n",
        "1. Context Matrix:   [vocab_size, embedding_dim]\n",
        "2. Target Matrix:  [vocab_size, embedding_dim]"
      ],
      "metadata": {
        "id": "vpKtfVMuXaMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(vocab_size, embedding_dim):\n",
        "  # Context Matrix\n",
        "  ####! add your code here\n",
        "  W1 =\n",
        "  ####\n",
        "\n",
        "  # Target Matrix\n",
        "  ####! add your code here\n",
        "  W2 =\n",
        "  ####\n",
        "\n",
        "  return W1, W2\n",
        "\n",
        "# visualization\n",
        "W1, W2 = initialize_parameters(vocab_size, embedding_dim)\n",
        "plot_embedding(W2, vocab)\n",
        "plot_cosine_similarity(W2, vocab)"
      ],
      "metadata": {
        "id": "tb2POyY3Xa4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Define Activation Function (5pt)\n",
        "Implement the sigmoid function used in the computation of prediction probabilities and gradients.\n",
        "\n",
        "\n",
        "Sigmoid Function: $\\sigma(z) = \\frac{1}{ 1 + exp(-z)}$"
      ],
      "metadata": {
        "id": "KLkINJYzXrtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! define sigmoid function\n",
        "def sigmoid(z):\n",
        "  z = np.clip(z, -100, 100)\n",
        "  ####! add your code here\n",
        "  return"
      ],
      "metadata": {
        "id": "vv3oMeEIXthZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Define Negative Sampling (15pt)\n",
        "\n",
        "Randomly select a set of negative word samples for each training instance, ensuring they differ from the current positive pair.\n",
        "\n",
        "By using negative sampling, we can improve computational efficiency by approximating the full softmax over the vocabulary.\n"
      ],
      "metadata": {
        "id": "wp3CIrUuX0RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_sampling(word_to_index, target_index, context_index, num_neg_samples):\n",
        "  \"\"\"\n",
        "  word_to_index: vocabulary.\n",
        "  target_index: target word index.\n",
        "  num_neg_samples: numbers of negative samples.\n",
        "  \"\"\"\n",
        "  vocab_indices = list(word_to_index.values())\n",
        "  # remove context and target tokens.\n",
        "  ####! add your code here\n",
        "\n",
        "  ####\n",
        "  # randomly choose negative words.\n",
        "  ####! add your code here\n",
        "  neg_samples =\n",
        "  ####\n",
        "\n",
        "  return neg_samples\n",
        "\n",
        "\n",
        "# test Negative Sampling\n",
        "neg_samples = negative_sampling(word_to_index, word_to_index[\"UCR\"], word_to_index[\"UCLA\"], num_neg_samples)\n",
        "print('Negative Words:')\n",
        "for sample in neg_samples:\n",
        "  print(index_to_word[sample])"
      ],
      "metadata": {
        "id": "w32PlyxkX5Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Forward (20pt)\n",
        "Compute the positive and negative prediction scores and calculate the loss using the negative sampling technique.\n",
        "\n",
        "Loss Function:\n",
        "$L_{CE}=-\\bigg[log\\sigma(c_{pos}\\cdot w) + \\sum_{i=1}^{k}log\\sigma(-c_{neg_i}\\cdot w)\\bigg]$"
      ],
      "metadata": {
        "id": "0RHZKryzZSD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_negative_sampling(context_word_index, target_word_index, neg_samples, W1, W2):\n",
        "  # obtain context vector\n",
        "  h_context = W1[context_word_index]\n",
        "  # obtain negative sample vector\n",
        "  ####! Add your code here\n",
        "  h_neg =\n",
        "  ####\n",
        "\n",
        "  # ompute positive prediction and loss.\n",
        "  u_target = np.dot(h_context, W2[target_word_index])\n",
        "  ####! Add your code here\n",
        "  pos_loss =\n",
        "  ####\n",
        "\n",
        "  # compute negative prediction and loss.\n",
        "  ####! Add your code here\n",
        "  u_neg_samples =\n",
        "  neg_loss =\n",
        "  ####\n",
        "\n",
        "  # total loss\n",
        "  loss = pos_loss + neg_loss\n",
        "  return u_target, u_neg_samples, loss"
      ],
      "metadata": {
        "id": "QuEeAPYJaJwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.Backward (20pt)\n",
        "Update model parameters using gradients derived from the loss function for both positive and negative samples.\n",
        "\n",
        "Loss Function:\n",
        "\n",
        "1. $c^{t+1}_{pos} = c^t_{pos} - \\eta [\\sigma(c^t_{pos}\\cdot w^t)-1]w^t$\n",
        "\n",
        "2. $c^{t+1}_{neg} = c^t_{neg} - \\eta [\\sigma(c^t_{neg}\\cdot w^t)]w^t$\n",
        "\n",
        "3. $w^{t+1} = w^t - \\eta \\bigg[ [\\sigma (c_{pos} \\cdot w^t - 1)]c_{pos} + \\sum_{i=1}^{k}[\\sigma (c_{neg_i}\\cdot w^t)] c_{neg_i}\\bigg]$"
      ],
      "metadata": {
        "id": "18DfnSMIaMVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_negative_sampling(\n",
        "      u_target,\n",
        "      u_neg_samples,\n",
        "      context_word_index,\n",
        "      target_word_index,\n",
        "      neg_samples_idx,\n",
        "      W1,\n",
        "      W2,\n",
        "      learning_rate):\n",
        "  \"\"\"\n",
        "  u_target: positive prediction.\n",
        "  u_negative_samples: negative predictions.\n",
        "  target_word_index: positive prediction word.\n",
        "  neg_samples_idx: negative prediction words.\n",
        "  W1: Embedding matrix.\n",
        "  W2: Projection matrix.\n",
        "  \"\"\"\n",
        "  # positive gradient.\n",
        "  grad_pos = sigmoid(u_target) - 1\n",
        "  W2[target_word_index] -= learning_rate * grad_pos * W1[context_word_index]\n",
        "  ####! Add your code here\n",
        "  W1[context_word_index] -=\n",
        "  ####\n",
        "\n",
        "  # negative gradient.\n",
        "  ####! Add your code here\n",
        "  grad_neg =\n",
        "  ####\n",
        "\n",
        "  for i, neg_index in enumerate(neg_samples_idx):\n",
        "      ####! Add your code here\n",
        "      W2[target_word_index] -=\n",
        "      W1[neg_index] -=\n",
        "      ####"
      ],
      "metadata": {
        "id": "ZQ8Puy74aO1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.Train Skip-Gram Model (20pt)\n",
        "\n",
        "Loop through the corpus over multiple epochs, applying forward and backward passes to learn word embeddings. You can train the model to produce meaningful word embeddings that capture contextual similarities.\n"
      ],
      "metadata": {
        "id": "LZMdEe3vbhdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_skip_gram_negative_sampling(corpus, word_to_index, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples):\n",
        "  \"\"\"\n",
        "  corpus: Training corpus.\n",
        "  word_to_index: vocabulary.\n",
        "  vocab_size: Size of vocabulary.\n",
        "  embedding_dim: Dimension of Embedding matrix.\n",
        "  learning_rate: learning rate for optimization.\n",
        "  epochs: Training epochs.\n",
        "  num_neg_samples: numbers of negative samples used during training.\n",
        "  \"\"\"\n",
        "  # init parameters\n",
        "  W1, W2 = initialize_parameters(vocab_size, embedding_dim)\n",
        "  total_loss = []\n",
        "  for epoch in range(epochs):\n",
        "      loss_epoch = 0\n",
        "      for context_word, target_word in corpus:\n",
        "\n",
        "          # convert to index.\n",
        "          context_word_index = word_to_index[context_word]\n",
        "          target_word_index =\n",
        "\n",
        "          # Obtain negative samples.\n",
        "          ####! Add your code here\n",
        "          neg_samples_index =\n",
        "          ####\n",
        "\n",
        "          # Forward\n",
        "          ####! Add your code here\n",
        "          u_target, u_neg_samples, loss =\n",
        "          ####\n",
        "          loss_epoch += loss\n",
        "\n",
        "          # Backward\n",
        "          ####! Add your code here\n",
        "\n",
        "          ####\n",
        "\n",
        "      avg_loss = loss_epoch / len(corpus)\n",
        "      print(f\"{STUDENT_NAME} Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
        "      total_loss.append(avg_loss)\n",
        "\n",
        "  return W1, W2, total_loss"
      ],
      "metadata": {
        "id": "1Absc3ihbl42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.Run Skip-Gram Model\n",
        "\n",
        "Executes the training process and visualizes the final word embeddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uNo8yHCzbugE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "W1, W2, total_loss = train_skip_gram_negative_sampling(corpus, word_to_index, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples)\n",
        "plot_embedding(W1, vocab)\n",
        "plot_loss_over_epochs(total_loss, epochs)\n",
        "plot_cosine_similarity(W2, vocab)"
      ],
      "metadata": {
        "id": "an62s1iobx-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 100\n",
        "W1, W2, total_loss = train_skip_gram_negative_sampling(corpus, word_to_index, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples)\n",
        "plot_embedding(W1, vocab)\n",
        "plot_loss_over_epochs(total_loss, epochs)\n",
        "plot_cosine_similarity(W2, vocab)"
      ],
      "metadata": {
        "id": "XPJNx-0qnSqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "W1, W2, total_loss = train_skip_gram_negative_sampling(corpus, word_to_index, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples)\n",
        "plot_embedding(W1, vocab)\n",
        "plot_loss_over_epochs(total_loss, epochs)\n",
        "plot_cosine_similarity(W2, vocab)"
      ],
      "metadata": {
        "id": "0sOdtDm1kf1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.Use Pre-trained Word2Vector Model (10pt)\n",
        "\n",
        "In this section, you should use pretrained GloVe word embeddings from the gensim library to compute the semantic similarity between sentences.\n",
        "\n"
      ],
      "metadata": {
        "id": "PLQtRm9x7bC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim.downloader\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# List all pretrained embedding models\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "\n",
        "# Download the \"glove-twitter-25\" embeddings\n",
        "models = gensim.downloader.load('glove-twitter-25')"
      ],
      "metadata": {
        "id": "4thmyUH47egx",
        "outputId": "67e3b8a2-8027-41b6-d53b-1bcf99858718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-44719e0da085>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_vector(review_text):\n",
        "    # Tokenize and clean the review\n",
        "    words = word_tokenize(review_text.lower())\n",
        "    words = [word for word in words if word not in string.punctuation]\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    # Remove words not in vocabulary\n",
        "    ####! Add your code here\n",
        "\n",
        "    ####\n",
        "    if not valid_words:\n",
        "        return np.zeros(models.vector_size)\n",
        "\n",
        "    # Average word vectors\n",
        "    ####! Add your code here\n",
        "    avg_vec =\n",
        "    ####\n",
        "    return avg_vev\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ],
      "metadata": {
        "id": "WH7sTMvd77Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Three Sentences.\n",
        "sent1 = \"The cat is sleeping on the sofa.\"\n",
        "sent2 = \"A cat is napping on the couch.\"\n",
        "sent3 = \"The spaceship landed on Mars after a six-month journey.\"\n",
        "\n",
        "# Convert reviews to vectors\n",
        "vec1 = review_to_vector(sent1)\n",
        "vec2 = review_to_vector(sent2)\n",
        "vec3 = review_to_vector(sent3)\n",
        "\n",
        "# Calculate similarities\n",
        "sim1 = cosine_similarity(vec1, vec2)\n",
        "sim2 = cosine_similarity(vec1, vec3)\n",
        "\n",
        "print(f\"sent1-sent2 similarity: {sim1:.4f}\")\n",
        "print(f\"sent1-sent3 similarity: {sim2:.4f}\")"
      ],
      "metadata": {
        "id": "2a7qkjnp8JgA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}